{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/10 19:55:24 WARN Utils: Your hostname, MacBook-Air-di-Davide.local resolves to a loopback address: 127.0.0.1; using 192.168.1.192 instead (on interface en0)\n",
      "24/01/10 19:55:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/10 19:55:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import year, month, count, row_number, when, col, regexp_replace, substring, udf, to_date, avg\n",
    "from pyspark.sql.functions import round as round_df\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "import geopy.distance\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"databases_project\").getOrCreate()\n",
    "df = spark.read.csv(\"Crime_Data_from_2010_to_2019.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.192:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>databases_project</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x127f1e200>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a DataFrame that contains the main data-set. Keep the original column names but change the column types as instructed below:\n",
    "• Date Rptd: date\n",
    "• DATE OCC: date\n",
    "• Vict Age: integer\n",
    "• LAT: double\n",
    "• LON: double\n",
    "Print the total number of rows for the entire data-set and the data type of every column.\n",
    "(5%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2118997\n",
      "root\n",
      " |-- DR_NO: integer (nullable = true)\n",
      " |-- Date Rptd: date (nullable = true)\n",
      " |-- DATE OCC: date (nullable = true)\n",
      " |-- TIME OCC: integer (nullable = true)\n",
      " |-- AREA : integer (nullable = true)\n",
      " |-- AREA NAME: string (nullable = true)\n",
      " |-- Rpt Dist No: integer (nullable = true)\n",
      " |-- Part 1-2: integer (nullable = true)\n",
      " |-- Crm Cd: integer (nullable = true)\n",
      " |-- Crm Cd Desc: string (nullable = true)\n",
      " |-- Mocodes: string (nullable = true)\n",
      " |-- Vict Age: integer (nullable = true)\n",
      " |-- Vict Sex: string (nullable = true)\n",
      " |-- Vict Descent: string (nullable = true)\n",
      " |-- Premis Cd: integer (nullable = true)\n",
      " |-- Premis Desc: string (nullable = true)\n",
      " |-- Weapon Used Cd: integer (nullable = true)\n",
      " |-- Weapon Desc: string (nullable = true)\n",
      " |-- Status: string (nullable = true)\n",
      " |-- Status Desc: string (nullable = true)\n",
      " |-- Crm Cd 1: integer (nullable = true)\n",
      " |-- Crm Cd 2: integer (nullable = true)\n",
      " |-- Crm Cd 3: integer (nullable = true)\n",
      " |-- Crm Cd 4: integer (nullable = true)\n",
      " |-- LOCATION: string (nullable = true)\n",
      " |-- Cross Street: string (nullable = true)\n",
      " |-- LAT: float (nullable = true)\n",
      " |-- LON: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"Date Rptd\", to_date(\"Date Rptd\", \"MM/dd/yyyy hh:mm:ss a\"))\n",
    "df = df.withColumn(\"DATE OCC\", to_date(\"DATE OCC\", \"MM/dd/yyyy hh:mm:ss a\"))\n",
    "df = df.withColumn(\"Vict Age\", df[\"Vict Age\"].cast(\"int\"))\n",
    "df = df.withColumn(\"LAT\", df[\"LAT\"].cast(\"float\"))\n",
    "df = df.withColumn(\"LON\", df[\"LON\"].cast(\"float\"))\n",
    "print(df.count())\n",
    "df.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Implement Query 1 using both the DataFrame and SQL APIs. Execute both implementations with 4 Spark executors. Do you notice differences in the execution times? Justify your answer. (15%)\n",
    "Find, for each year, the top-3 months with highest number of recorded crimes committed. You are asked to print the month, year, number of criminal acts recorded, as well as the ranking of the month within the respective year. Results are expected to be sorted in ascending order with respect to the year and descending order with respect to the number of crimes (as in Table 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:======================>                                    (3 + 5) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----------+---+\n",
      "|Month|Year|crime_total|  #|\n",
      "+-----+----+-----------+---+\n",
      "|    7|2010|       6037|  1|\n",
      "|   10|2010|       6035|  2|\n",
      "|    3|2010|       6032|  3|\n",
      "|    3|2011|      14953|  1|\n",
      "|    5|2011|      14896|  2|\n",
      "|    4|2011|      14396|  3|\n",
      "|    1|2012|      31423|  1|\n",
      "|    8|2012|      31041|  2|\n",
      "|   10|2012|      30921|  3|\n",
      "|    1|2013|       8691|  1|\n",
      "|    8|2013|       8008|  2|\n",
      "|   12|2013|       8001|  3|\n",
      "|    5|2014|       5296|  1|\n",
      "|    6|2014|       5248|  2|\n",
      "|    7|2014|       4830|  3|\n",
      "|    3|2015|      10200|  1|\n",
      "|    5|2015|      10018|  2|\n",
      "|    7|2015|       9785|  3|\n",
      "|   12|2016|      16670|  1|\n",
      "|   10|2016|      16616|  2|\n",
      "+-----+----+-----------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Group by year and month, count the number of crimes\n",
    "df_grouped = df.groupBy(year(\"DATE OCC\").alias(\"Year\"), month(\"DATE OCC\").alias(\"Month\")) \\\n",
    "               .agg(count(\"*\").alias(\"crime_total\"))\n",
    "\n",
    "# Rank the months within each year based on the number of crimes\n",
    "window_spec = Window.partitionBy(\"Year\").orderBy(df_grouped[\"crime_total\"].desc())\n",
    "df_ranked = df_grouped.withColumn(\"#\", row_number().over(window_spec))\n",
    "\n",
    "# Filter for the top-3 months with the highest number of crimes for each year\n",
    "df_top3 = df_ranked.filter(df_ranked[\"#\"] <= 3)\n",
    "\n",
    "# Sort the results in ascending order with respect to the year and descending order with respect to the number of crimes\n",
    "df_sorted = df_top3.orderBy(\"Year\", df_top3[\"crime_total\"].desc())\n",
    "\n",
    "# Print the month, year, number of criminal acts recorded, and ranking of the month within the respective year\n",
    "df_sorted.select(\"Month\", \"Year\", \"crime_total\", \"#\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/10 19:55:33 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 11:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----------+----+\n",
      "|Year|Month|crime_total|Rank|\n",
      "+----+-----+-----------+----+\n",
      "|2010|    7|       6037|   1|\n",
      "|2010|   10|       6035|   2|\n",
      "|2010|    3|       6032|   3|\n",
      "|2011|    3|      14953|   1|\n",
      "|2011|    5|      14896|   2|\n",
      "|2011|    4|      14396|   3|\n",
      "|2012|    1|      31423|   1|\n",
      "|2012|    8|      31041|   2|\n",
      "|2012|   10|      30921|   3|\n",
      "|2013|    1|       8691|   1|\n",
      "|2013|    8|       8008|   2|\n",
      "|2013|   12|       8001|   3|\n",
      "|2014|    5|       5296|   1|\n",
      "|2014|    6|       5248|   2|\n",
      "|2014|    7|       4830|   3|\n",
      "|2015|    3|      10200|   1|\n",
      "|2015|    5|      10018|   2|\n",
      "|2015|    7|       9785|   3|\n",
      "|2016|   12|      16670|   1|\n",
      "|2016|   10|      16616|   2|\n",
      "+----+-----+-----------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"df\")\n",
    "sql_df = spark.sql(\"\"\"\n",
    "SELECT Year, Month, crime_total, Rank\n",
    "FROM (\n",
    "    SELECT Year, Month, crime_total, ROW_NUMBER() OVER (PARTITION BY Year ORDER BY crime_total DESC) AS Rank\n",
    "    FROM (\n",
    "        SELECT YEAR(`DATE OCC`) AS Year, MONTH(`DATE OCC`) AS Month, COUNT(*) AS crime_total\n",
    "        FROM df\n",
    "        GROUP BY Year, Month\n",
    "    ) t\n",
    ") t\n",
    "WHERE Rank <= 3\n",
    "ORDER BY Year ASC, crime_total DESC\n",
    "\"\"\")\n",
    "sql_df.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement Query 2 using both the DataFrame/SQL and RDD APIs. Report and compare execution times for 4 Spark executors. (20%)\n",
    "Sort the different parts of the day taking into account crimes that were committed on the (STREET), in descending order. Consider the following pars of the day:\n",
    "• Morning: 5.00am – 11.59am • Afternoon: 12.00pm – 4.59pm\n",
    "• Evening: 5.00pm – 8.59pm • Night: 9.00pm – 3.59am"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "| Day_Part|crime_total|\n",
      "+---------+-----------+\n",
      "|    Night|     169568|\n",
      "|  Evening|     131422|\n",
      "|Afternoon|     104408|\n",
      "|  Morning|      87648|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Dividing the \"Time OCC\" column into 4 parts of the day\n",
    "df_day_parts = df.withColumn(\"Day_Part\",\n",
    "                             when((col(\"Time OCC\") >= 500) & (col(\"Time OCC\") < 1200), \"Morning\")\n",
    "                             .when((col(\"Time OCC\") >= 1200) & (col(\"Time OCC\") < 1700), \"Afternoon\")\n",
    "                             .when((col(\"Time OCC\") >= 1700) & (col(\"Time OCC\") < 2100), \"Evening\")\n",
    "                             .otherwise(\"Night\"))\n",
    "\n",
    "# Group by the day parts and count the number of crimes\n",
    "df_sorted = df_day_parts.filter(col(\"Premis Desc\") == \"STREET\").groupBy(col(\"Day_Part\")).agg(count(\"*\").alias(\"crime_total\")).orderBy(col(\"crime_total\").desc())\n",
    "df_sorted.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Evening', 131422)\n",
      "('Morning', 87648)\n",
      "('Night', 169568)\n",
      "('Afternoon', 104408)\n"
     ]
    }
   ],
   "source": [
    "# Filter the RDD to include only crimes committed on the street\n",
    "rdd_filtered = df.rdd.filter(lambda row: row[\"Premis Desc\"] == \"STREET\")\n",
    "\n",
    "# Map each crime record to a day part\n",
    "rdd_mapped = rdd_filtered.map(lambda row: (\n",
    "    \"Morning\" if 500 <= row[\"TIME OCC\"] < 1200 else\n",
    "    \"Afternoon\" if 1200 <= row[\"TIME OCC\"] < 1700 else\n",
    "    \"Evening\" if 1700 <= row[\"TIME OCC\"] < 2100 else\n",
    "    \"Night\"\n",
    ")\n",
    ")\n",
    "\n",
    "# Map day parts to a tuple containing key and 1, reduce by key to aggregate the counts for each day part\n",
    "rdd_reduced = rdd_mapped.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Sort the resulting RDD in descending order based on the count\n",
    "rdd_sorted = rdd_reduced.sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "# Print the sorted RDD\n",
    "for record in rdd_reduced.collect():\n",
    "    print(record)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Implement Query 3 using the DataFrame/SQL API. Report and compare execution times for 2, 3 and 4 Spark executors. (20%)\n",
    "Find the descent of the victims of recorded crimes in Los Angeles for the year 2015 in the 3 ZIP Code areas with the highest and the 3 ZIP Codes with the lowest income per household. Results are expected to be printed from highest to lowest number of victims per ethnic group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading additional dataframe\n",
    "income2015_df = spark.read.csv(\"income/LA_income_2015.csv\", header=True, inferSchema=True)\n",
    "# Converting from string format to integer\n",
    "income2015_df = income2015_df.withColumn(\"Estimated Median Income\", regexp_replace(\"Estimated Median Income\", \"\\\\$\", \"\"))\n",
    "income2015_df = income2015_df.withColumn(\"Estimated Median Income\", regexp_replace(\"Estimated Median Income\", \",\", \"\").cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting dataframe by descending order of income\n",
    "income2015_df_sorted = income2015_df.sort(col(\"Estimated Median Income\").desc())\n",
    "# Selecting top 3 and bottom 3 rows\n",
    "highest_rows = income2015_df_sorted.select(\"Zip Code\").head(3)\n",
    "lowest_rows = income2015_df_sorted.select(\"Zip Code\").tail(3)\n",
    "highest_zipcodes = [row[\"Zip Code\"] for row in highest_rows]\n",
    "lowest_zipcodes = [row[\"Zip Code\"] for row in lowest_rows]\n",
    "zipcodes = highest_zipcodes + lowest_zipcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading additional dataframe needed for reverse geocoding\n",
    "geo_df = spark.read.csv(\"revgecoding.csv\", header=True, inferSchema=True)\n",
    "geo_df = geo_df.withColumn(\"ZIPcode\", substring(col(\"ZIPcode\"), 0, 5).cast(\"int\"))\n",
    "geo_df = geo_df.withColumn(\"LAT\", geo_df[\"LAT\"].cast(\"float\"))\n",
    "geo_df = geo_df.withColumn(\"LON\", geo_df[\"LON\"].cast(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|        Vict Descent|  #|\n",
      "+--------------------+---+\n",
      "|Hispanic/Latin/Me...|118|\n",
      "|               White|102|\n",
      "|               Black| 99|\n",
      "|             Unknown| 33|\n",
      "+--------------------+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---+\n",
      "|Vict Descent|  #|\n",
      "+------------+---+\n",
      "+------------+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 42:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|        Vict Descent|  #|\n",
      "+--------------------+---+\n",
      "|Hispanic/Latin/Me...|118|\n",
      "|               White|102|\n",
      "|               Black| 99|\n",
      "|             Unknown| 33|\n",
      "+--------------------+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Filter the DataFrame for the year 2015\n",
    "df_2015 = df.filter(year(\"DATE OCC\") == 2015)\n",
    "\n",
    "# Filter the victemless crimes\n",
    "df_2015 = df_2015.filter(df_2015[\"Vict Age\"] > 0)\n",
    "\n",
    "# Join the DataFrame with the reverse geocoding DataFrame\n",
    "df_2015 = df_2015.join(geo_df, on=[\"LAT\", \"LON\"], how=\"left\")\n",
    "\n",
    "# Map column \"Vict Descent\" to required format\n",
    "df_2015 = df_2015.withColumn(\"Vict Descent\", \\\n",
    "                            when(df_2015[\"Vict Descent\"] == \"W\", \"White\") \\\n",
    "                            .when(df_2015[\"Vict Descent\"] == \"B\", \"Black\") \\\n",
    "                            .when(df_2015[\"Vict Descent\"] == \"H\", \"Hispanic/Latin/Mexican\") \\\n",
    "                            .when(df_2015[\"Vict Descent\"] == \"L\", \"Hispanic/Latin/Mexican\") \\\n",
    "                            .when(df_2015[\"Vict Descent\"] == \"M\", \"Hispanic/Latin/Mexican\") \\\n",
    "                            .otherwise(\"Unknown\")\n",
    ")\n",
    "\n",
    "# Filter the DataFrame for the top-3 and bottom-3 zip codes and both\n",
    "df_2015_all_zipcodes = df_2015.filter(df_2015[\"ZIPCode\"].isin(zipcodes))\n",
    "df_2015_high = df_2015.filter(df_2015[\"ZIPCode\"].isin(highest_zipcodes))\n",
    "df_2015_low = df_2015.filter(df_2015[\"ZIPCode\"].isin(lowest_zipcodes))\n",
    "\n",
    "# Group by the victim descent and count the number of crimes\n",
    "df_2015_all_zipcodes.groupBy(\"Vict Descent\").agg(count(\"*\").alias(\"#\")).sort(col(\"#\").desc()).show()\n",
    "df_2015_high.groupBy(\"Vict Descent\").agg(count(\"*\").alias(\"#\")).sort(col(\"#\").desc()).show()\n",
    "df_2015_low.groupBy(\"Vict Descent\").agg(count(\"*\").alias(\"#\")).sort(col(\"#\").desc()).show()\n",
    "\n",
    "# We also did the same thing using geopy but it was too slow\n",
    "# def get_zipcode(lat, lon):\n",
    "#     geolocator = Nominatim(user_agent=\"my_geocoder\")\n",
    "#     location = geolocator.reverse((lat, lon), exactly_one=True)\n",
    "#     if location:\n",
    "#         address = location.raw.get(\"address\")\n",
    "#         if address:\n",
    "#             return int(address.get(\"postcode\"))\n",
    "#     return None\n",
    "\n",
    "# get_zipcode_udf = udf(get_zipcode, IntegerType())\n",
    "\n",
    "# df_2015 = df_2015.withColumn(\"Zip Code\", get_zipcode_udf(df_2015[\"LAT\"], df_2015[\"LON\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement Query 4 using the DataFrame/SQL API. (20%)\n",
    "For the last query, you are asked to examine whether the police stations that respond to recorded crimes in Los Angeles are the closest ones to the crime scene. Towards that end, you are asked to implement and execute two pairs of similar queries and compare results:\n",
    "- Calculate the number of crimes committed with the use of firearms of any kind and the average distance (in km) of the crime scene to the police station that handled the case. The results should appear ordered by year in ascending order. Additionally, calculate the same stats (number of crimes committed with the use of firearms of any kind and average distance) per police station. Results should appear ordered by number of incidents, in descending order (as in Table 3).\n",
    "- Calculate the number of crimes committed with the use of firearms of any kind and the average distance (in km) of the crime scene to the police station that is located closest to the crime scene. The results should appear ordered by year in ascending order. Additionally, calculate the same stats (number of crimes committed with the use of firearms of any kind and average distance) per police station. Results should appear ordered by number of incidents, in descending order (as in Table 4).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- FID: integer (nullable = true)\n",
      " |-- DIVISION: string (nullable = true)\n",
      " |-- LOCATION: string (nullable = true)\n",
      " |-- PREC: integer (nullable = true)\n",
      " |-- Police_LON: float (nullable = true)\n",
      " |-- Police_LAT: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading additional dataframe containing police stations\n",
    "police_df = spark.read.csv(\"LAPD_Police_Stations.csv\", header=True, inferSchema=True)\n",
    "police_df = police_df.withColumn(\"Police_LON\", police_df[\"X\"].cast(\"float\").alias(\"Police_LON\"))\n",
    "police_df = police_df.withColumn(\"Police_LAT\", police_df[\"Y\"].cast(\"float\").alias(\"Police_LAT\"))\n",
    "police_df = police_df.drop(\"X\", \"Y\")\n",
    "police_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+---------------------+\n",
      "|Year|Number of Crimes|Average Distance (km)|\n",
      "+----+----------------+---------------------+\n",
      "|2010|            2686|                 2.56|\n",
      "|2011|            5855|                 2.75|\n",
      "|2012|           12021|                2.836|\n",
      "|2013|            2271|                2.731|\n",
      "|2014|            2320|                2.684|\n",
      "|2015|            3500|                2.653|\n",
      "|2016|            6537|                2.682|\n",
      "|2017|            9890|                2.764|\n",
      "|2018|            2242|                2.565|\n",
      "|2019|            7129|                2.739|\n",
      "|2021|           10175|                2.695|\n",
      "|2022|            9884|                2.709|\n",
      "|2023|            2820|                2.862|\n",
      "+----+----------------+---------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 52:==============>                                           (2 + 6) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------+---------------------+\n",
      "|       DIVISION|Number of Crimes|Average Distance (km)|\n",
      "+---------------+----------------+---------------------+\n",
      "|    77TH STREET|           12704|                2.677|\n",
      "|      SOUTHEAST|            8373|                2.097|\n",
      "|         NEWTON|            7908|                2.019|\n",
      "|      SOUTHWEST|            6301|                2.689|\n",
      "|     HOLLENBECK|            4346|                2.719|\n",
      "|         HARBOR|            3881|                4.077|\n",
      "|        RAMPART|            3335|                1.631|\n",
      "|      NORTHEAST|            2967|                3.876|\n",
      "|        OLYMPIC|            2897|                1.829|\n",
      "|        MISSION|            2877|                4.666|\n",
      "|       FOOTHILL|            2618|                3.852|\n",
      "|      HOLLYWOOD|            2599|                1.447|\n",
      "|       WILSHIRE|            2472|                2.333|\n",
      "|NORTH HOLLYWOOD|            2449|                2.741|\n",
      "|    WEST VALLEY|            2207|                3.581|\n",
      "|       VAN NUYS|            1861|                2.193|\n",
      "|        PACIFIC|            1821|                3.742|\n",
      "|     DEVONSHIRE|            1791|                3.977|\n",
      "|        CENTRAL|            1480|                1.234|\n",
      "|        TOPANGA|            1323|                3.438|\n",
      "+---------------+----------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def get_distance (lat1 , lon1 , lat2 , lon2 ) :\n",
    "    return round(geopy.distance.distance((lat1, lon1), (lat2, lon2)).km, 3)\n",
    "\n",
    "# Defining a UDF to calculate distance between two points\n",
    "get_distance_udf = udf(get_distance, FloatType())\n",
    "\n",
    "# Filtering values with 0 latitude and longitude \n",
    "df_null_island = df.where((col(\"LAT\") != 0) & (col(\"LON\") != 0))\n",
    "# Filtering values with weapon code between 100 and 200 i.e. guns\n",
    "df_guns = df_null_island.where((col(\"Weapon Used Cd\") >= 100) & (col(\"Weapon Used Cd\") < 200))\n",
    "# Joining guns dataframe with police dataframe\n",
    "df_guns_police = df_guns.join(police_df, [df_guns[\"AREA \"] == police_df[\"PREC\"]], how=\"left\")\n",
    "# Selecting useful columns\n",
    "df_crimes = df_guns_police.select(\"LAT\", \"LON\", \"Police_LAT\", \"Police_LON\", \"PREC\", \"DATE OCC\", \"DIVISION\")\n",
    "\n",
    "# Calculating distance between crime location and police station that responded to the crime\n",
    "df_crimes = df_crimes.withColumn(\"Distance\", get_distance_udf(df_crimes[\"LAT\"], df_crimes[\"LON\"], df_crimes[\"Police_LAT\"], df_crimes[\"Police_LON\"]))\n",
    "\n",
    "# Grouping by year and calculating average distance\n",
    "df_stats1 = df_crimes.groupBy(year(\"DATE OCC\").alias(\"Year\")).agg(count(\"*\").alias(\"Number of Crimes\"), avg(\"Distance\").alias(\"Average Distance (km)\")).orderBy(\"Year\")\n",
    "# Rounding the average distance to 3 decimal places\n",
    "df_stats1 = df_stats1.withColumn(\"Average Distance (km)\", round_df(df_stats1[\"Average Distance (km)\"], 3))\n",
    "\n",
    "# Grouping by division and calculating average distance\n",
    "df_stats2 = df_crimes.groupBy(\"DIVISION\").agg(count(\"*\").alias(\"Number of Crimes\"), avg(\"Distance\").alias(\"Average Distance (km)\")).orderBy(col(\"Number of Crimes\").desc())\n",
    "# Rounding the average distance to 3 decimal places\n",
    "df_stats2 = df_stats2.withColumn(\"Average Distance (km)\", round_df(df_stats2[\"Average Distance (km)\"], 3))\n",
    "\n",
    "df_stats1.show()\n",
    "df_stats2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Repeating the same steps as above but instead the police station with the closest distance is selected\n",
    "police_cross_join = police_df.crossJoin(df_guns)\n",
    "# Calculating distance between crime location and closest police station\n",
    "police_cross_join = police_cross_join.withColumn(\"Distance\", get_distance_udf(police_cross_join[\"LAT\"], police_cross_join[\"LON\"], police_cross_join[\"Police_LAT\"], police_cross_join[\"Police_LON\"]))\n",
    "window_spec = Window.partitionBy(\"DR_NO\").orderBy(col(\"Distance\").asc())\n",
    "closest_police_station_df = police_cross_join.withColumn(\"row_num\", F.row_number().over(window_spec)).filter(\"row_num = 1\").drop(\"row_num\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+---------------------+\n",
      "|Year|Number of Crimes|Average Distance (km)|\n",
      "+----+----------------+---------------------+\n",
      "|2010|            2686|                2.271|\n",
      "|2011|            5579|                2.409|\n",
      "|2012|            6532|                2.505|\n",
      "|2013|            2271|                2.415|\n",
      "|2014|            2320|                 2.16|\n",
      "|2015|            3500|                 2.46|\n",
      "|2016|            6076|                2.414|\n",
      "|2017|            7786|                2.392|\n",
      "|2018|            2242|                2.348|\n",
      "|2019|            7129|                2.429|\n",
      "|2021|            8483|                2.481|\n",
      "|2022|            6963|                2.404|\n",
      "|2023|            2820|                2.596|\n",
      "+----+----------------+---------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 63:===========================================>              (6 + 2) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------+---------------------+\n",
      "|       DIVISION|Number of Crimes|Average Distance (km)|\n",
      "+---------------+----------------+---------------------+\n",
      "|    77TH STREET|            7468|                1.717|\n",
      "|      SOUTHWEST|            7275|                2.228|\n",
      "|      SOUTHEAST|            6363|                2.206|\n",
      "|     HOLLENBECK|            4376|                2.703|\n",
      "|       WILSHIRE|            4256|                 2.46|\n",
      "|         HARBOR|            3800|                 3.91|\n",
      "|         NEWTON|            3688|                1.578|\n",
      "|      HOLLYWOOD|            3550|                 1.97|\n",
      "|        OLYMPIC|            3137|                1.676|\n",
      "|        RAMPART|            2913|                1.419|\n",
      "|       VAN NUYS|            2656|                2.955|\n",
      "|       FOOTHILL|            2487|                3.616|\n",
      "|NORTH HOLLYWOOD|            2001|                2.737|\n",
      "|      NORTHEAST|            1851|                3.736|\n",
      "|        CENTRAL|            1588|                 1.05|\n",
      "|    WEST VALLEY|            1541|                2.805|\n",
      "|        MISSION|            1505|                3.811|\n",
      "|        TOPANGA|            1327|                3.029|\n",
      "|        PACIFIC|            1222|                3.755|\n",
      "|     DEVONSHIRE|             747|                2.988|\n",
      "+---------------+----------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculating the same stats as above\n",
    "df_stats11 = closest_police_station_df.groupBy(year(\"DATE OCC\").alias(\"Year\")).agg(count(\"*\").alias(\"Number of Crimes\"), avg(\"Distance\").alias(\"Average Distance (km)\")).orderBy(\"Year\")\n",
    "\n",
    "df_stats11 = df_stats11.withColumn(\"Average Distance (km)\", round_df(df_stats11[\"Average Distance (km)\"], 3))\n",
    "\n",
    "df_stats22 = closest_police_station_df.groupBy(\"DIVISION\").agg(count(\"*\").alias(\"Number of Crimes\"), avg(\"Distance\").alias(\"Average Distance (km)\")).orderBy(col(\"Number of Crimes\").desc())\n",
    "\n",
    "df_stats22 = df_stats22.withColumn(\"Average Distance (km)\", round_df(df_stats22[\"Average Distance (km)\"], 3))\n",
    "\n",
    "df_stats11.show()\n",
    "df_stats22.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
